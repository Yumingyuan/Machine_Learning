{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降法\n",
    "## 首先我们给出正常的梯度下降方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=100000\n",
    "x=np.random.normal(size=m)\n",
    "X=x.reshape(m,1)\n",
    "y=4. * x + 3. +np.random.normal(0,3,size=m)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正常的拟合\n",
    "def J(theta,X_b,y):\n",
    "    try:\n",
    "        return np.sum((y-X_b.dot(theta))**2)/X_b.shape[0]\n",
    "    except:\n",
    "        return float('inf')\n",
    "def dJ(theta,X_b,y):\n",
    "    return 2. * X_b.T.dot(X_b.dot(theta)-y)/X_b.shape[0]\n",
    "def gradient_descent(X_b,y,initial_theta,eta,n_iters=1e4,epsilon=1e-8):#增加一个限制循环次数的情况，避免死循环\n",
    "    theta=initial_theta\n",
    "    i_iter=0 \n",
    "    while i_iter<n_iters:\n",
    "        gradient = dJ(theta,X_b,y)#theta点的导数\n",
    "        last_theta=theta\n",
    "        theta = theta - eta * gradient#theta移动\n",
    "        \n",
    "        if abs(J(theta,X_b,y)-J(last_theta,X_b,y))<epsilon:#函数变化情况很小\n",
    "            break#退出循环不找了，再找也小不了到多少了\n",
    "        i_iter+=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.00938111, 3.99533618])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X_b=np.hstack([np.ones((X.shape[0],1)),X])\n",
    "initial_theta=np.zeros(X_b.shape[1])\n",
    "eta=0.01\n",
    "theta=gradient_descent(X_b,y,initial_theta,eta)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_sgd(theta,X_b_i,y_i):#随机梯度下降X_b_i是X_b中的任意一行，y_i是y中的任意一行\n",
    "    return 2. * X_b_i.T.dot(X_b_i.dot(theta)-y_i) / X_b_i.shape[0]#由于是一行，所以X_b_i.shape[0]==1，相当于除以1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X_b,y,initial_theta,n_iters):#随机梯度下降法\n",
    "    t0=5#模拟退火的参数，分子\n",
    "    t1=50#模拟退火的参数，分母\n",
    "    \n",
    "    def learning_rate(t):#t为下降的迭代次数\n",
    "        return t0/(t+t1)\n",
    "    theta= initial_theta\n",
    "    for cur_iter in range(n_iters):#迭代次数\n",
    "        rand_i=np.random.randint(X_b.shape[0])#随机生成一个int，作为选取样本的索引\n",
    "        X_b_rand_i=X_b[rand_i,:]#取出rand_i对应的样本\n",
    "        y_rand_i=y[rand_i]\n",
    "        gradient=dJ_sgd(theta,X_b_rand_i,y_rand_i)\n",
    "        theta= theta - learning_rate(cur_iter)*gradient#根据样本计算的梯度，向反方向梯度下降一次\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666.6666666666667 1666\n"
     ]
    }
   ],
   "source": [
    "print(5000/3,5000//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_b=np.hstack([np.ones((X.shape[0],1)),X])\n",
    "initial_theta=np.zeros(X_b.shape[1])\n",
    "theta=sgd(X_b,y,initial_theta,n_iters=X_b.shape[0]//3)#只需要1/3的样本即可得到不错的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.99222031, 4.03160543])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta#随机梯度后发现，其实和实际结果差不多，且只用了1/3的样本数就得到了这个不错的值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
